# Qwen3-8B æ–°é—»æ¨èæ¨¡å‹å¾®è°ƒé¡¹ç›®

<div align="center">

![Python Version](https://img.shields.io/badge/Python-3.8%2B-blue)
![Model](https://img.shields.io/badge/Model-Qwen3--8B-green)
![Framework](https://img.shields.io/badge/Framework-LLaMA--Factory-orange)
![License](https://img.shields.io/badge/License-Apache%202.0-yellow)

**åŸºäº Qwen3-8B å’Œ MIND æ•°æ®é›†çš„æ–°é—»æ¨èç³»ç»Ÿ | ä½¿ç”¨ LoRA å¾®è°ƒ | é›†æˆ vLLM é«˜æ€§èƒ½æ¨ç†**

[English](README_EN.md) | ç®€ä½“ä¸­æ–‡

</div>

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#-é¡¹ç›®ç®€ä»‹)
- [æ ¸å¿ƒç‰¹æ€§](#-æ ¸å¿ƒç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [æ•°æ®è¯´æ˜](#-æ•°æ®è¯´æ˜)
- [æ¨¡å‹å¾®è°ƒè¯¦è§£](#-æ¨¡å‹å¾®è°ƒè¯¦è§£)
- [è®­ç»ƒæµç¨‹](#-è®­ç»ƒæµç¨‹)
- [æ¨¡å‹è¯„ä¼°](#-æ¨¡å‹è¯„ä¼°)
- [ä½¿ç”¨æŒ‡å—](#-ä½¿ç”¨æŒ‡å—)
- [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
- [æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜faq)
- [å‚è€ƒèµ„æ–™](#-å‚è€ƒèµ„æ–™)
- [è®¸å¯è¯](#-è®¸å¯è¯)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®åŸºäº **Qwen3-8B** å¤§è¯­è¨€æ¨¡å‹å’Œ **MIND (Microsoft News Dataset)** æ•°æ®é›†,æ„å»ºäº†ä¸€ä¸ªæ–°é—»æ¨èç³»ç»Ÿã€‚é€šè¿‡ **LoRA (Low-Rank Adaptation)** å¾®è°ƒæ–¹æ³•,å°†æ–°é—»æ¨èä»»åŠ¡è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡,é¢„æµ‹ç”¨æˆ·æ˜¯å¦ä¼šç‚¹å‡»ç‰¹å®šæ–°é—»ã€‚

### é¡¹ç›®äº®ç‚¹

- ğŸš€ **é«˜æ•ˆå¾®è°ƒ**: ä½¿ç”¨ LoRA æ–¹æ³•,ä»…è®­ç»ƒ 0.5% çš„å‚æ•°,æ˜¾è‘—é™ä½è®­ç»ƒæˆæœ¬
- âš¡ **æé€Ÿæ¨ç†**: é›†æˆ vLLM å®ç° **18-25 å€æ¨ç†åŠ é€Ÿ**
- ğŸ“Š **å®Œæ•´æµç¨‹**: ä»æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒåˆ°è¯„ä¼°çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ
- ğŸ“ **æ•™è‚²å‹å¥½**: è¯¦ç»†çš„æ–‡æ¡£å’Œä»£ç ç¤ºä¾‹,é€‚åˆå­¦ä¹ å’Œç ”ç©¶
- ğŸ”§ **çµæ´»é…ç½®**: æ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³•åˆ‡æ¢ (LoRA/Full/P-Tuning/QLoRA)

### æŠ€æœ¯æ ˆ

```
åŸºåº§æ¨¡å‹: Qwen3-8B (8B å‚æ•°)
å¾®è°ƒæ¡†æ¶: LLaMA-Factory
å¾®è°ƒæ–¹æ³•: LoRA (Low-Rank Adaptation)
æ¨ç†å¼•æ“: vLLM (é«˜æ€§èƒ½æ¨ç†)
æ•°æ®é›†: MIND (Microsoft News Dataset)
è¯­è¨€: Python 3.10+
```

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### 1. æ•°æ®å¤„ç†
- âœ… å®Œæ•´çš„ MIND æ•°æ®é›†å¤„ç†æµç¨‹ (train/val/test)
- âœ… æ™ºèƒ½æ ·æœ¬æ„å»ºç­–ç•¥ (ç”¨æˆ·å†å²ã€å–œå¥½æ–°é—»ã€ç›®æ ‡æ–°é—»)
- âœ… ç”Ÿæˆçº¦ **125,000** è®­ç»ƒæ ·æœ¬å’Œ **31,000** æµ‹è¯•æ ·æœ¬

### 2. æ¨¡å‹å¾®è°ƒ
- âœ… æ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³•: **LoRA**, Full Fine-tuning, P-Tuning v2, QLoRA
- âœ… å‚æ•°é«˜æ•ˆ: LoRA ä»…è®­ç»ƒ **0.5%** å‚æ•°,é€‚é…å™¨ä»… **81 MB**
- âœ… æ˜¾å­˜ä¼˜åŒ–: æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ (BF16), æ¢¯åº¦ç´¯ç§¯

### 3. é«˜æ€§èƒ½æ¨ç†
- âœ… **vLLM æ¨ç†**: 10-30 samples/sec, 2-5 åˆ†é’Ÿè¯„ä¼° 1000 æ ·æœ¬
- âœ… **API æ¨ç†**: å¿«é€ŸéªŒè¯, æ”¯æŒ OpenAI SDK å…¼å®¹æ¥å£
- âœ… **æ€§èƒ½æå‡**: vLLM æ¯” API æ–¹å¼å¿« **18-25 å€**

### 4. å…¨é¢è¯„ä¼°
- âœ… åŸºåº§æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹å¯¹æ¯”
- âœ… å‡†ç¡®ç‡ã€æ¨ç†é€Ÿåº¦ã€ååé‡ç­‰å¤šç»´åº¦æŒ‡æ ‡
- âœ… é”™è¯¯æ¡ˆä¾‹åˆ†æå’Œå¯è§†åŒ–æŠ¥å‘Š

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

| ç»„ä»¶ | è¦æ±‚ | è¯´æ˜ |
|------|------|------|
| **Python** | 3.10+ | æ¨èä½¿ç”¨ Anaconda |
| **CUDA** | 12.2+ | GPU åŠ é€Ÿè®­ç»ƒå’Œæ¨ç† |
| **æ˜¾å­˜** | 24GB+ | LoRA å¾®è°ƒæœ€ä½è¦æ±‚ |
| **æ˜¾å­˜ (æ¨è)** | 40GB+ | ç”¨äº LoRA æƒé‡åˆå¹¶å’Œ vLLM æ¨ç† |
| **ç£ç›˜ç©ºé—´** | 60GB+ | æ¨¡å‹ (28GB) + æ•°æ®é›† + åˆå¹¶æ¨¡å‹ (28GB) |

### ä¾èµ–å®‰è£…

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (æ¨è)
conda create -n qwen_news python=3.10
conda activate qwen_news

# 2. å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 3. å®‰è£… LLaMA-Factory
pip install llmtuner

# 4. å®‰è£…å…¶ä»–ä¾èµ–
pip install modelscope  # æ¨¡å‹ä¸‹è½½
pip install peft        # LoRA æƒé‡åˆå¹¶
pip install vllm        # é«˜æ€§èƒ½æ¨ç†
pip install jupyter     # Jupyter Notebook
pip install pandas numpy scikit-learn  # æ•°æ®å¤„ç†
```

### ä¸€é”®è¿è¡Œ

```bash
# æ­¥éª¤ 1: ä¸‹è½½ Qwen3-8B æ¨¡å‹ (é¦–æ¬¡è¿è¡Œ, çº¦ 28GB)
python download_qwen_model.py

# æ­¥éª¤ 2: æ•°æ®é¢„å¤„ç†
jupyter notebook "æ•°æ®é¢„å¤„ç†.ipynb"

# æ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ (çº¦ 3 epochs)
llamafactory-cli train new_train.yaml

# æ­¥éª¤ 4: åˆå¹¶ LoRA æƒé‡ (vLLM æ¨ç†å¿…éœ€)
python merge_lora_weights.py

# æ­¥éª¤ 5: è¯„ä¼°æ¨¡å‹ (æ¨èä½¿ç”¨ vLLM, 18-25å€åŠ é€Ÿ)
jupyter notebook "æ¨¡å‹å¯¹æ¯”è¯„ä¼°_vllm.ipynb"
```

---

## ğŸ“Š æ•°æ®è¯´æ˜

### MIND æ•°æ®é›†ä»‹ç»

[MIND (Microsoft News Dataset)](https://msnews.github.io/) æ˜¯å¾®è½¯å‘å¸ƒçš„å¤§è§„æ¨¡æ–°é—»æ¨èæ•°æ®é›†,åŒ…å«çœŸå®çš„ç”¨æˆ·ç‚¹å‡»è¡Œä¸ºæ•°æ®ã€‚

#### æ•°æ®é›†è§„æ¨¡

| æ•°æ®é›† | æ–°é—»æ•°é‡ | ç”¨æˆ·è¡Œä¸ºæ•°é‡ | è¯´æ˜ |
|--------|---------|-------------|------|
| **Train** | 101,527 | 2,232,748 | è®­ç»ƒé›† |
| **Val** | - | - | éªŒè¯é›† |
| **Test** | 120,961 | 2,370,727 | æµ‹è¯•é›† |

#### æ•°æ®ç›®å½•ç»“æ„

```
data/
â”œâ”€â”€ MIND/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ news.tsv        # æ–°é—»å…ƒæ•°æ® (NewsID, Category, Title, etc.)
â”‚   â”‚   â””â”€â”€ behaviors.tsv   # ç”¨æˆ·è¡Œä¸º (UserID, Time, History, Impressions)
â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”œâ”€â”€ news.tsv
â”‚   â”‚   â””â”€â”€ behaviors.tsv
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ news.tsv
â”‚       â””â”€â”€ behaviors.tsv
â””â”€â”€ processed/
    â”œâ”€â”€ mind_train.json     # å¤„ç†åçš„è®­ç»ƒæ ·æœ¬ (~125,000æ¡)
    â”œâ”€â”€ mind_val.json       # å¤„ç†åçš„éªŒè¯æ ·æœ¬ (~35,000æ¡)
    â””â”€â”€ mind_test.json      # å¤„ç†åçš„æµ‹è¯•æ ·æœ¬ (~31,000æ¡)
```

#### åŸå§‹æ•°æ®æ ¼å¼

**news.tsv** (æ–°é—»å…ƒæ•°æ®)
```
NewsID    Category    SubCategory    Title                           Abstract    ...
N12345    sports      football       "Team A wins championship"      "..."       ...
N67890    tech        AI             "New AI model released"         "..."       ...
```

**behaviors.tsv** (ç”¨æˆ·è¡Œä¸º)
```
UserID    Time              History                    Impressions
U1001     2020-10-15 10:30  N12345 N67890 N11111      N22222-1 N33333-0 N44444-1
U1002     2020-10-15 10:31  N55555 N66666             N77777-0 N88888-1
```

- **History**: ç”¨æˆ·ç‚¹å‡»å†å² (ç©ºæ ¼åˆ†éš”çš„ NewsID)
- **Impressions**: æ›å…‰æ–°é—»åˆ—è¡¨ (æ ¼å¼: NewsID-Label, 1=ç‚¹å‡», 0=æœªç‚¹å‡»)

### æ•°æ®å¤„ç†æµç¨‹

#### 1. è¯»å–åŸå§‹æ•°æ®

```python
import pandas as pd

# è¯»å–æ–°é—»æ•°æ®
news_df = pd.read_csv(
    'data/MIND/train/news.tsv',
    sep='\t',
    names=['news_id', 'category', 'subcategory', 'title', 'abstract',
           'url', 'title_entities', 'abstract_entities']
)

# æ„å»ºæ–°é—» ID åˆ°æ ‡é¢˜çš„æ˜ å°„
news_dict = dict(zip(news_df['news_id'], news_df['title']))

# è¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
behaviors_df = pd.read_csv(
    'data/MIND/train/behaviors.tsv',
    sep='\t',
    names=['impression_id', 'user_id', 'time', 'history', 'impressions']
)
```

#### 2. æ ·æœ¬æ„å»ºç­–ç•¥

æ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«ä»¥ä¸‹ä¿¡æ¯:

```python
sample = {
    "instruction": "You are a news recommendation expert. Based on user's click history and preferences, predict if they will click the target news.",
    "input": f"""User click histories: "{hist1}", "{hist2}", "{hist3}"
User's liked news: "{liked_news1}", "{liked_news2}"
User's disliked news: "{disliked_news1}", "{disliked_news2}"
Target news: "{target_news}"
Will the user click this news? Answer with 'Yes.' or 'No.'""",
    "output": "Yes."  # æˆ– "No."
}
```

**æ ·æœ¬æ„å»ºè§„åˆ™:**
- **ç”¨æˆ·å†å²**: ä¿ç•™æœ€è¿‘ **3 æ¡**ç‚¹å‡»æ–°é—» (å¦‚æœè¶…è¿‡ 3 æ¡)
- **å–œæ¬¢çš„æ–°é—»**: ä»æ›å…‰åˆ—è¡¨ä¸­æ ‡ç­¾ä¸º `1` çš„æ–°é—» (ç‚¹å‡»è¿‡çš„)
- **ä¸å–œæ¬¢çš„æ–°é—»**: ä»æ›å…‰åˆ—è¡¨ä¸­æ ‡ç­¾ä¸º `0` çš„æ–°é—» (æœªç‚¹å‡»çš„)
- **ç›®æ ‡æ–°é—»**: æ›å…‰åˆ—è¡¨çš„**æœ€åä¸€ä¸ª**æ–°é—»ä½œä¸ºé¢„æµ‹ç›®æ ‡

#### 3. æ ·æœ¬ç”Ÿæˆä»£ç ç‰‡æ®µ

```python
def build_training_sample(user_history, impressions, news_dict):
    """æ„å»ºå•ä¸ªè®­ç»ƒæ ·æœ¬"""
    # é™åˆ¶ç”¨æˆ·å†å²é•¿åº¦
    history_list = user_history.split()[-3:]  # æœ€å¤šä¿ç•™ 3 æ¡
    history_titles = [news_dict.get(nid, "") for nid in history_list]

    # è§£ææ›å…‰åˆ—è¡¨
    impression_list = impressions.split()
    liked_news, disliked_news = [], []

    for imp in impression_list[:-1]:  # é™¤äº†æœ€åä¸€ä¸ª
        news_id, label = imp.split('-')
        title = news_dict.get(news_id, "")
        if label == '1':
            liked_news.append(title)
        else:
            disliked_news.append(title)

    # æœ€åä¸€ä¸ªä½œä¸ºç›®æ ‡
    target_news_id, target_label = impression_list[-1].split('-')
    target_title = news_dict.get(target_news_id, "")

    # æ„å»ºæ ·æœ¬
    sample = {
        "instruction": "You are a news recommendation expert...",
        "input": f"""User click histories: {', '.join([f'"{t}"' for t in history_titles])}
User's liked news: {', '.join([f'"{t}"' for t in liked_news[:3]])}
User's disliked news: {', '.join([f'"{t}"' for t in disliked_news[:3]])}
Target news: "{target_title}"
Will the user click this news? Answer with 'Yes.' or 'No.'""",
        "output": "Yes." if target_label == '1' else "No."
    }

    return sample
```

#### 4. å¤„ç†åçš„æ•°æ®ç»Ÿè®¡

| æ•°æ®é›† | æ ·æœ¬æ•°é‡ | æ–‡ä»¶å¤§å° | æ­£æ ·æœ¬æ¯”ä¾‹ | å¹³å‡å†å²é•¿åº¦ |
|--------|---------|---------|-----------|-------------|
| **Train** | ~125,000 | 753 KB | ~50% | 2.8 |
| **Val** | ~35,000 | - | ~50% | 2.7 |
| **Test** | ~31,000 | 188 KB | ~50% | 2.9 |

---

## ğŸ”§ æ¨¡å‹å¾®è°ƒè¯¦è§£

### åŸºåº§æ¨¡å‹: Qwen3-8B

[Qwen3-8B](https://github.com/QwenLM/Qwen) æ˜¯é˜¿é‡Œå·´å·´é€šä¹‰åƒé—®å›¢é˜Ÿå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚

| å‚æ•° | è¯´æ˜ |
|------|------|
| **æ¨¡å‹å‚æ•°** | 8B (140 äº¿) |
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 128K tokens |
| **æ¨¡å‹å¤§å°** | ~28 GB (FP16) |
| **è®­ç»ƒæ•°æ®** | å¤šé¢†åŸŸé«˜è´¨é‡ä¸­è‹±æ–‡æ•°æ® |

#### æ¨¡å‹ä¸‹è½½

```bash
# æ–¹å¼ 1: ä½¿ç”¨é¡¹ç›®æä¾›çš„ä¸‹è½½è„šæœ¬ (æ¨è)
python download_qwen_model.py

# æ–¹å¼ 2: ä½¿ç”¨ ModelScope CLI
pip install modelscope
modelscope download --model Qwen/Qwen3-8B --local_dir ./Qwen/Qwen3-8B

# ä¸‹è½½å®Œæˆå,æ¨¡å‹ä¿å­˜åœ¨ ./Qwen/Qwen3-8B/ (çº¦ 28GB)
```

### å¾®è°ƒæ–¹æ³•å¯¹æ¯”

æœ¬é¡¹ç›®æ”¯æŒå¤šç§å¾®è°ƒæ–¹æ³•,ä¸‹è¡¨å¯¹æ¯”äº†å„æ–¹æ³•çš„ç‰¹ç‚¹:

| å¾®è°ƒæ–¹æ³• | å¯è®­ç»ƒå‚æ•° | æ˜¾å­˜å ç”¨ (è®­ç»ƒ) | è®­ç»ƒé€Ÿåº¦ | æ¨ç†é€Ÿåº¦ | æ¨èåœºæ™¯ | æ•ˆæœ |
|---------|-----------|---------------|---------|---------|---------|------|
| **Full Fine-tuning** | 100% (8B) | 50GB+ | æ…¢ (1x) | å¿« | å¤§è§„æ¨¡æ•°æ®, å……è¶³èµ„æº | â­â­â­â­â­ |
| **LoRA** âœ… | ~0.5% (70M) | 24GB | å¿« (3x) | å¿« | **èµ„æºå—é™, æ•ˆæœè¦æ±‚é«˜** | â­â­â­â­â­ |
| **P-Tuning v2** | ~0.1% (10M) | 20GB | å¾ˆå¿« (5x) | å¿« | å¿«é€Ÿå®éªŒ, è½»é‡çº§ä»»åŠ¡ | â­â­â­â­ |
| **Adapter** | ~2% (280M) | 28GB | ä¸­ç­‰ (2x) | å¿« | å¤šä»»åŠ¡å­¦ä¹  | â­â­â­â­ |
| **QLoRA (4-bit)** | ~0.5% (70M) | 12GB | ä¸­ç­‰ (2x) | ä¸­ç­‰ | **æ˜¾å­˜æåº¦å—é™** | â­â­â­â­ |

âœ… **æœ¬é¡¹ç›®ä½¿ç”¨ LoRA**, å¹³è¡¡äº†æ•ˆæœå’Œæ•ˆç‡

---

### æ–¹æ³• 1: LoRA (Low-Rank Adaptation) â­ æ¨è

#### åŸç†ä»‹ç»

LoRA é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µæ—è¾¹æ·»åŠ **ä½ç§©åˆ†è§£çŸ©é˜µ**æ¥å®ç°é«˜æ•ˆå¾®è°ƒ:

```
åŸå§‹æƒé‡æ›´æ–°: W' = W + Î”W
LoRA åˆ†è§£: Î”W = B Ã— A  (å…¶ä¸­ B: dÃ—r, A: rÃ—k, r << min(d,k))
```

- **r**: LoRA rank, æ§åˆ¶å‚æ•°é‡ (æœ¬é¡¹ç›®ä½¿ç”¨ r=8)
- **å‚æ•°é‡**: ä»…ä¸ºå…¨å‚æ•°å¾®è°ƒçš„ **0.5%**
- **ä¼˜åŠ¿**: è®­ç»ƒå¿«ã€æ˜¾å­˜å°‘ã€æ•ˆæœå¥½ã€æ˜“äºéƒ¨ç½²

#### LoRA é…ç½®

åœ¨ [new_train.yaml](new_train.yaml) ä¸­çš„é…ç½®:

```yaml
# å¾®è°ƒæ–¹æ³•
finetuning_type: lora

# LoRA ç›®æ ‡æ¨¡å— (Qwen3-8B çš„ 7 ä¸ªæ ¸å¿ƒå±‚)
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# LoRA å‚æ•° (åœ¨ LLaMA-Factory ä¸­è‡ªåŠ¨è®¾ç½®)
# lora_rank: 8           # LoRA ç§©
# lora_alpha: 16         # ç¼©æ”¾å› å­
# lora_dropout: 0.0      # Dropout ç‡
```

#### å®Œæ•´è®­ç»ƒé…ç½®ç¤ºä¾‹

```yaml
### æ¨¡å‹é…ç½®
model_name_or_path: ./Qwen/Qwen3-8B
template: qwen

### å¾®è°ƒæ–¹æ³•
stage: sft
do_train: true
finetuning_type: lora
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

### æ•°æ®é›†é…ç½®
dataset: mind
dataset_dir: ./
dataset_info: dataset_info.json
cutoff_len: 1024
max_samples: 1000  # å¯è°ƒæ•´æˆ–åˆ é™¤ä»¥ä½¿ç”¨å…¨éƒ¨æ•°æ®
overwrite_cache: true
preprocessing_num_workers: 16

### è®­ç»ƒå‚æ•°
per_device_train_batch_size: 1
gradient_accumulation_steps: 8  # æœ‰æ•ˆæ‰¹å¤§å° = 1 Ã— 8 = 8
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true  # æ··åˆç²¾åº¦è®­ç»ƒ

### è¯„ä¼°é…ç½®
val_size: 0.1  # è‡ªåŠ¨åˆ’åˆ† 10% ä½œä¸ºéªŒè¯é›†
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500

### è¾“å‡ºé…ç½®
output_dir: ./saves/qwen3_mind_news_recommend
logging_steps: 10
save_steps: 500
plot_loss: true
```

#### LoRA ä¼˜åŠ¿æ€»ç»“

âœ… **å‚æ•°é«˜æ•ˆ**: ä»…è®­ç»ƒ 0.5% å‚æ•° (70M vs 8B)
âœ… **æ˜¾å­˜å‹å¥½**: 24GB æ˜¾å­˜å³å¯è®­ç»ƒ 8B æ¨¡å‹
âœ… **è®­ç»ƒå¿«é€Ÿ**: æ¯”å…¨å‚æ•°å¾®è°ƒå¿« **3-5 å€**
âœ… **æ•ˆæœä¼˜å¼‚**: æ¥è¿‘å…¨å‚æ•°å¾®è°ƒçš„æ•ˆæœ
âœ… **æ˜“äºéƒ¨ç½²**: é€‚é…å™¨ä»… 81 MB, å¯å¿«é€Ÿåˆ‡æ¢

---

### æ–¹æ³• 2: Full Fine-tuning (å…¨å‚æ•°å¾®è°ƒ)

#### é€‚ç”¨åœºæ™¯

- âœ… æœ‰å……è¶³çš„è®¡ç®—èµ„æº (50GB+ æ˜¾å­˜)
- âœ… æ•°æ®é›†è§„æ¨¡è¾ƒå¤§ (100K+ æ ·æœ¬)
- âœ… è¿½æ±‚æœ€ä½³æ•ˆæœ
- âœ… ä»»åŠ¡ä¸é¢„è®­ç»ƒå·®å¼‚è¾ƒå¤§

#### é…ç½®æ–¹æ³•

ä¿®æ”¹ `new_train.yaml`:

```yaml
### å¾®è°ƒæ–¹æ³•
stage: sft
do_train: true
finetuning_type: full  # æ”¹ä¸º full

# åˆ é™¤ LoRA ç›¸å…³é…ç½®
# lora_target: ...  (åˆ é™¤)

### è®­ç»ƒå‚æ•° (éœ€è¦è°ƒæ•´)
per_device_train_batch_size: 1
gradient_accumulation_steps: 16  # å¢åŠ ç´¯ç§¯æ­¥æ•°
learning_rate: 5.0e-6  # é™ä½å­¦ä¹ ç‡
num_train_epochs: 2.0  # å‡å°‘è®­ç»ƒè½®æ•°
```

#### ä¸ LoRA å¯¹æ¯”

| æŒ‡æ ‡ | Full Fine-tuning | LoRA |
|------|-----------------|------|
| **è®­ç»ƒå‚æ•°** | 8B | 70M |
| **æ˜¾å­˜å ç”¨** | 50GB+ | 24GB |
| **è®­ç»ƒæ—¶é—´** | åŸºå‡† (1x) | 3-5å€å¿« |
| **é€‚é…å™¨å¤§å°** | 28GB | 81MB |
| **æ•ˆæœ** | æœ€å¥½ (100%) | ä¼˜ç§€ (95-98%) |

---

### æ–¹æ³• 3: P-Tuning v2

#### åŸç†å’Œä¼˜åŠ¿

P-Tuning v2 ä»…åœ¨æ¨¡å‹çš„**æ¯ä¸€å±‚è¾“å…¥**æ·»åŠ å¯è®­ç»ƒçš„è¿ç»­æç¤ºå‘é‡:

- **å¯è®­ç»ƒå‚æ•°**: ~0.1% (çº¦ 10M)
- **æ˜¾å­˜å ç”¨**: 20GB
- **è®­ç»ƒé€Ÿåº¦**: æ¯” LoRA æ›´å¿«
- **é€‚ç”¨åœºæ™¯**: å¿«é€Ÿå®éªŒã€è½»é‡çº§ä»»åŠ¡

#### é…ç½®ç¤ºä¾‹

```yaml
### å¾®è°ƒæ–¹æ³•
finetuning_type: p_tuning
pre_seq_len: 128  # æç¤ºå‘é‡é•¿åº¦
prefix_projection: true  # ä½¿ç”¨æŠ•å½±å±‚
```

#### ä¼˜ç¼ºç‚¹

âœ… **ä¼˜ç‚¹**: å‚æ•°æœ€å°‘, è®­ç»ƒæœ€å¿«, æ˜¾å­˜å ç”¨ä½
âŒ **ç¼ºç‚¹**: æ•ˆæœç•¥ä½äº LoRA (çº¦ 90-95%)

---

### æ–¹æ³• 4: Adapter

#### å®ç°æ–¹å¼

åœ¨ Transformer çš„æ¯ä¸€å±‚æ·»åŠ **è½»é‡çº§é€‚é…å™¨æ¨¡å—**:

```
Transformer Layer:
  Self-Attention â†’ Adapter â†’ Feed-Forward â†’ Adapter
```

- **å¯è®­ç»ƒå‚æ•°**: ~2% (çº¦ 280M)
- **æ˜¾å­˜å ç”¨**: 28GB
- **é€‚ç”¨åœºæ™¯**: å¤šä»»åŠ¡å­¦ä¹ , éœ€è¦å¿«é€Ÿåˆ‡æ¢ä¸åŒä»»åŠ¡

#### é…ç½®ç¤ºä¾‹

```yaml
### å¾®è°ƒæ–¹æ³•
finetuning_type: adapter
adapter_size: 64  # é€‚é…å™¨éšè—å±‚å¤§å°
```

---

### æ–¹æ³• 5: QLoRA (é‡åŒ– LoRA)

#### åŸç†å’Œä¼˜åŠ¿

QLoRA ç»“åˆäº†**é‡åŒ–æŠ€æœ¯**å’Œ LoRA:

- åŸºåº§æ¨¡å‹é‡åŒ–ä¸º **4-bit** æˆ– **8-bit**
- LoRA æƒé‡ä¿æŒ FP16/BF16
- **æ˜¾å­˜å ç”¨**: 12GB (4-bit)
- **æ•ˆæœ**: ä¸ LoRA ç›¸å½“ (95-98%)

#### é…ç½®ç¤ºä¾‹

```yaml
### å¾®è°ƒæ–¹æ³•
finetuning_type: lora
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

### é‡åŒ–é…ç½®
quantization_bit: 4  # 4-bit é‡åŒ–
quantization_method: bitsandbytes
```

#### ä¼˜ç¼ºç‚¹

âœ… **ä¼˜ç‚¹**: æ˜¾å­˜å ç”¨æä½ (12GB), é€‚åˆæ¶ˆè´¹çº§æ˜¾å¡
âŒ **ç¼ºç‚¹**: æ¨ç†é€Ÿåº¦ç•¥æ…¢, é‡åŒ–ç²¾åº¦æŸå¤±

---

### å¾®è°ƒæ–¹æ³•é€‰æ‹©æŒ‡å—

| å¦‚æœä½ ... | æ¨èæ–¹æ³• | åŸå›  |
|---------|---------|------|
| æ˜¾å­˜ >= 40GB, è¿½æ±‚æœ€ä½³æ•ˆæœ | **Full Fine-tuning** | æ•ˆæœæœ€å¥½ |
| æ˜¾å­˜ 24-40GB, å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡ | **LoRA** â­ | æœ¬é¡¹ç›®é€‰æ‹© |
| æ˜¾å­˜ 12-24GB, æ˜¾å­˜å—é™ | **QLoRA (4-bit)** | æä½æ˜¾å­˜ |
| éœ€è¦å¿«é€Ÿå®éªŒéªŒè¯ | **P-Tuning v2** | è®­ç»ƒæœ€å¿« |
| å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ | **Adapter** | æ˜“äºåˆ‡æ¢ |

---

## ğŸ‹ï¸ è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒæ­¥éª¤

#### æ­¥éª¤ 1: å‡†å¤‡æ•°æ®

```bash
# 1. ç¡®ä¿ MIND æ•°æ®é›†å·²æ”¾ç½®åœ¨ data/MIND/ ç›®å½•
ls data/MIND/train/  # åº”åŒ…å« news.tsv å’Œ behaviors.tsv

# 2. è¿è¡Œæ•°æ®é¢„å¤„ç†
jupyter notebook "æ•°æ®é¢„å¤„ç†.ipynb"

# 3. éªŒè¯å¤„ç†åçš„æ•°æ®
ls data/processed/  # åº”åŒ…å« mind_train.json, mind_test.json
```

#### æ­¥éª¤ 2: é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ [new_train.yaml](new_train.yaml) è°ƒæ•´å‚æ•°:

```yaml
# å…³é”®å‚æ•°è¯´æ˜
max_samples: 1000        # è®­ç»ƒæ ·æœ¬æ•° (åˆ é™¤æ­¤è¡Œä½¿ç”¨å…¨éƒ¨æ•°æ®)
num_train_epochs: 3.0    # è®­ç»ƒè½®æ•°
learning_rate: 1.0e-4    # å­¦ä¹ ç‡
```

#### æ­¥éª¤ 3: å¯åŠ¨è®­ç»ƒ

```bash
# ä½¿ç”¨ LLaMA-Factory CLI å¯åŠ¨è®­ç»ƒ
llamafactory-cli train new_train.yaml

# è®­ç»ƒè¿‡ç¨‹è¾“å‡ºç¤ºä¾‹:
# [INFO] Loading model from ./Qwen/Qwen3-8B
# [INFO] Using LoRA with rank 8
# [INFO] Trainable params: 70M / 8B (0.5%)
# [INFO] Training started...
# Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [10:30<00:00, loss=0.352]
# Eval loss: 0.298
# ...
```

#### æ­¥éª¤ 4: ç›‘æ§è®­ç»ƒ

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:

```
saves/qwen3_mind_news_recommend/
â”œâ”€â”€ adapter_model.safetensors  # LoRA æƒé‡ (81 MB)
â”œâ”€â”€ adapter_config.json        # LoRA é…ç½®
â”œâ”€â”€ trainer_state.json         # è®­ç»ƒçŠ¶æ€å’Œå†å²
â”œâ”€â”€ training_loss.png          # æŸå¤±æ›²çº¿å›¾ (å¦‚æœå¯ç”¨ plot_loss)
â””â”€â”€ checkpoint-500/            # ä¸­é—´æ£€æŸ¥ç‚¹ (æ¯ 500 æ­¥ä¿å­˜)
```

æŸ¥çœ‹è®­ç»ƒæ—¥å¿—:

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f saves/qwen3_mind_news_recommend/trainer_log.jsonl

# ç»˜åˆ¶æŸå¤±æ›²çº¿
python -c "
import json
import matplotlib.pyplot as plt

with open('saves/qwen3_mind_news_recommend/trainer_state.json') as f:
    state = json.load(f)

losses = [log['loss'] for log in state['log_history'] if 'loss' in log]
plt.plot(losses)
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.savefig('training_loss.png')
print('æŸå¤±æ›²çº¿å·²ä¿å­˜åˆ° training_loss.png')
"
```

### è®­ç»ƒè¶…å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | è°ƒä¼˜å»ºè®® |
|------|--------|------|---------|
| `learning_rate` | 1.0e-4 | å­¦ä¹ ç‡ | LoRA: 1e-4, Full: 5e-6 |
| `num_train_epochs` | 3.0 | è®­ç»ƒè½®æ•° | æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´ (2-5) |
| `per_device_train_batch_size` | 1 | æ¯å¼ å¡çš„æ‰¹å¤§å° | æ ¹æ®æ˜¾å­˜è°ƒæ•´ (1-4) |
| `gradient_accumulation_steps` | 8 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° | æœ‰æ•ˆæ‰¹å¤§å° = batch_size Ã— accumulation |
| `warmup_ratio` | 0.1 | é¢„çƒ­æ¯”ä¾‹ | 10% è®­ç»ƒæ­¥æ•°ç”¨äºé¢„çƒ­ |
| `lr_scheduler_type` | cosine | å­¦ä¹ ç‡è°ƒåº¦å™¨ | cosine, linear, constant |
| `max_samples` | 1000 | æœ€å¤§è®­ç»ƒæ ·æœ¬æ•° | åˆ é™¤ä»¥ä½¿ç”¨å…¨éƒ¨æ•°æ® |

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1: CUDA Out of Memory (OOM)

**é”™è¯¯ä¿¡æ¯**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# æ–¹æ³• 1: å‡å°‘æ‰¹å¤§å°
per_device_train_batch_size: 1  # é™ä½åˆ° 1
gradient_accumulation_steps: 16  # å¢åŠ ç´¯ç§¯æ­¥æ•°

# æ–¹æ³• 2: ä½¿ç”¨é‡åŒ–
quantization_bit: 4

# æ–¹æ³• 3: å‡å°‘ LoRA ç›®æ ‡æ¨¡å—
lora_target: q_proj,v_proj  # åªä¿ç•™ q_proj å’Œ v_proj

# æ–¹æ³• 4: é™ä½åºåˆ—é•¿åº¦
cutoff_len: 512  # ä» 1024 é™ä½åˆ° 512
```

#### é—®é¢˜ 2: è®­ç»ƒé€Ÿåº¦æ…¢

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
bf16: true  # æˆ– fp16: true

# 2. å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
preprocessing_num_workers: 16  # æ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´

# 3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (æ˜¾å­˜æ¢æ—¶é—´)
gradient_checkpointing: false  # è®¾ä¸º false åŠ é€Ÿ

# 4. ä½¿ç”¨æ›´å¿«çš„ä¼˜åŒ–å™¨
optim: adamw_torch_fused  # PyTorch èåˆä¼˜åŒ–å™¨
```

#### é—®é¢˜ 3: éªŒè¯æŸå¤±ä¸ä¸‹é™

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# 1. è°ƒæ•´å­¦ä¹ ç‡
learning_rate: 5.0e-5  # é™ä½å­¦ä¹ ç‡

# 2. è°ƒæ•´é¢„çƒ­æ¯”ä¾‹
warmup_ratio: 0.15  # å¢åŠ é¢„çƒ­æ­¥æ•°

# 3. æ£€æŸ¥æ•°æ®è´¨é‡
max_samples: 10000  # å¢åŠ è®­ç»ƒæ ·æœ¬æ•°

# 4. è°ƒæ•´ LoRA rank
# åœ¨ adapter_config.json ä¸­æ‰‹åŠ¨ä¿®æ”¹ r: 16  (å¢åŠ åˆ° 16)
```

---

## ğŸ“ˆ æ¨¡å‹è¯„ä¼°

æœ¬é¡¹ç›®æä¾›ä¸¤ç§è¯„ä¼°æ–¹å¼: **vLLM æ‰¹é‡æ¨ç†** (æ¨è) å’Œ **API æœåŠ¡æ¨ç†** (å¤‡é€‰)ã€‚

### è¯„ä¼°æ–¹å¼å¯¹æ¯”

| è¯„ä¼°æ–¹å¼ | ååé‡ | 1000æ ·æœ¬è€—æ—¶ | å…¨é‡31Kæ ·æœ¬è€—æ—¶ | æ˜¾å­˜å ç”¨ | é€‚ç”¨åœºæ™¯ |
|---------|-------|------------|--------------|---------|---------|
| **vLLM æ‰¹é‡æ¨ç†** â­ | 10-30 s/s | 2-5 åˆ†é’Ÿ | 25-50 åˆ†é’Ÿ | 28GB | æ‰¹é‡è¯„ä¼°ã€ç”Ÿäº§éƒ¨ç½² |
| **API æœåŠ¡æ¨ç†** | 0.3-0.5 s/s | 30-50 åˆ†é’Ÿ | 20+ å°æ—¶ | 24GB | å¿«é€ŸéªŒè¯ã€å°è§„æ¨¡æµ‹è¯• |

**åŠ é€Ÿå€æ•°**: vLLM æ¯” API æ–¹å¼å¿« **18-25 å€** ğŸš€

---

### æ–¹å¼ 1: vLLM æ‰¹é‡æ¨ç† â­ æ¨è

vLLM æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤§æ¨¡å‹æ¨ç†å¼•æ“,æ”¯æŒæ‰¹å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†,æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚

#### æ­¥éª¤ 1: åˆå¹¶ LoRA æƒé‡

âš ï¸ **é‡è¦**: vLLM æ¨ç†å‰å¿…é¡»å…ˆåˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹

```bash
# è¿è¡Œåˆå¹¶è„šæœ¬ (éœ€è¦çº¦ 40GB æ˜¾å­˜)
python merge_lora_weights.py

# åˆå¹¶è¿‡ç¨‹è¾“å‡º:
# ============================================================
# å¼€å§‹åˆå¹¶ LoRA æƒé‡
# åŸºåº§æ¨¡å‹: ./Qwen/Qwen3-8B
# LoRA é€‚é…å™¨: ./saves/qwen3_mind_news_recommend
# è¾“å‡ºè·¯å¾„: ./merged_models/qwen3_mind_news_recommend
# ============================================================
#
# [1/4] åŠ è½½ LoRA æ¨¡å‹...
# âœ“ LoRA æ¨¡å‹åŠ è½½æˆåŠŸ
#
# [2/4] åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹...
# âœ“ æƒé‡åˆå¹¶å®Œæˆ
#
# [3/4] ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ° ./merged_models/qwen3_mind_news_recommend...
# âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ
#
# [4/4] ä¿å­˜ tokenizer...
# âœ“ Tokenizer ä¿å­˜æˆåŠŸ
# ============================================================
```

**åˆå¹¶åçš„ç›®å½•ç»“æ„**:
```
merged_models/qwen3_mind_news_recommend/
â”œâ”€â”€ config.json                      # æ¨¡å‹é…ç½®
â”œâ”€â”€ model-00001-of-00008.safetensors # æ¨¡å‹æƒé‡ (åˆ†ç‰‡)
â”œâ”€â”€ model-00002-of-00008.safetensors
â”œâ”€â”€ ...
â”œâ”€â”€ tokenizer.json                   # åˆ†è¯å™¨
â””â”€â”€ tokenizer_config.json
```

#### æ­¥éª¤ 2: ä½¿ç”¨ vLLM è¯„ä¼°

æ‰“å¼€ [æ¨¡å‹å¯¹æ¯”è¯„ä¼°_vllm.ipynb](æ¨¡å‹å¯¹æ¯”è¯„ä¼°_vllm.ipynb):

```python
from vllm import LLM, SamplingParams
import json

# 1. åŠ è½½åˆå¹¶åçš„å¾®è°ƒæ¨¡å‹
print("åŠ è½½å¾®è°ƒæ¨¡å‹...")
finetuned_llm = LLM(
    model="./merged_models/qwen3_mind_news_recommend",
    trust_remote_code=True,
    gpu_memory_utilization=0.9,  # ä½¿ç”¨ 90% æ˜¾å­˜
    max_model_len=2048,
    dtype="bfloat16"
)

# 2. åŠ è½½åŸºåº§æ¨¡å‹ (ç”¨äºå¯¹æ¯”)
print("åŠ è½½åŸºåº§æ¨¡å‹...")
base_llm = LLM(
    model="./Qwen/Qwen3-8B",
    trust_remote_code=True,
    gpu_memory_utilization=0.9,
    max_model_len=2048,
    dtype="bfloat16"
)

# 3. åŠ è½½æµ‹è¯•æ•°æ®
with open("data/processed/mind_test.json", "r", encoding="utf-8") as f:
    test_data = [json.loads(line) for line in f][:1000]  # æµ‹è¯• 1000 æ ·æœ¬

# 4. æ„å»º prompts
def build_prompt(sample):
    """æ„å»º Qwen3 æ ¼å¼çš„ prompt"""
    return f"""<|im_start|>system
{sample['instruction']}<|im_end|>
<|im_start|>user
{sample['input']}<|im_end|>
<|im_start|>assistant
"""

prompts = [build_prompt(sample) for sample in test_data]

# 5. æ‰¹é‡æ¨ç† (vLLM è‡ªåŠ¨æ‰¹å¤„ç†)
sampling_params = SamplingParams(
    temperature=0.01,  # ä½æ¸©åº¦,æ¥è¿‘è´ªå¿ƒè§£ç 
    top_p=0.9,
    max_tokens=10,
    stop=["<|im_end|>", "\n"]
)

print("å¼€å§‹æ¨ç† (å¾®è°ƒæ¨¡å‹)...")
finetuned_outputs = finetuned_llm.generate(prompts, sampling_params)

print("å¼€å§‹æ¨ç† (åŸºåº§æ¨¡å‹)...")
base_outputs = base_llm.generate(prompts, sampling_params)

# 6. è§£æç»“æœå¹¶è®¡ç®—å‡†ç¡®ç‡
def parse_answer(text):
    """è§£ææ¨¡å‹è¾“å‡º"""
    text = text.strip().lower()
    if "yes" in text:
        return "Yes."
    elif "no" in text:
        return "No."
    return "Unknown"

# å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡
finetuned_correct = 0
for output, sample in zip(finetuned_outputs, test_data):
    pred = parse_answer(output.outputs[0].text)
    if pred == sample['output']:
        finetuned_correct += 1

# åŸºåº§æ¨¡å‹å‡†ç¡®ç‡
base_correct = 0
for output, sample in zip(base_outputs, test_data):
    pred = parse_answer(output.outputs[0].text)
    if pred == sample['output']:
        base_correct += 1

print(f"\n{'='*60}")
print(f"è¯„ä¼°ç»“æœ (1000 æ ·æœ¬)")
print(f"{'='*60}")
print(f"åŸºåº§æ¨¡å‹å‡†ç¡®ç‡: {base_correct/len(test_data)*100:.2f}%")
print(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {finetuned_correct/len(test_data)*100:.2f}%")
print(f"å‡†ç¡®ç‡æå‡: +{(finetuned_correct-base_correct)/len(test_data)*100:.2f}%")
print(f"{'='*60}")
```

#### vLLM æ€§èƒ½ä¼˜åŒ–å‚æ•°

```python
LLM(
    model="./merged_models/qwen3_mind_news_recommend",

    # æ˜¾å­˜ç®¡ç†
    gpu_memory_utilization=0.9,  # GPU æ˜¾å­˜åˆ©ç”¨ç‡ (0.8-0.95)
    max_model_len=2048,          # æœ€å¤§åºåˆ—é•¿åº¦

    # æ‰¹å¤„ç†å‚æ•°
    max_num_batched_tokens=8192, # æ‰¹å¤„ç† token ä¸Šé™
    max_num_seqs=256,            # æœ€å¤§å¹¶å‘åºåˆ—æ•°

    # æ€§èƒ½ä¼˜åŒ–
    dtype="bfloat16",            # æ•°æ®ç±»å‹ (bfloat16/float16)
    enforce_eager=False,         # ä½¿ç”¨ CUDA graph åŠ é€Ÿ
    trust_remote_code=True       # ä¿¡ä»»è‡ªå®šä¹‰ä»£ç 
)
```

---

### æ–¹å¼ 2: API æœåŠ¡æ¨ç†

é€‚ç”¨äºå¿«é€ŸéªŒè¯å’Œå°è§„æ¨¡æµ‹è¯•ã€‚

#### æ­¥éª¤ 1: å¯åŠ¨ API æœåŠ¡

```bash
# ç»ˆç«¯ 1: å¯åŠ¨åŸºåº§æ¨¡å‹æœåŠ¡ (ç«¯å£ 8000)
llamafactory-cli api \
  --model_name_or_path ./Qwen/Qwen3-8B \
  --template qwen \
  --port 8000

# ç»ˆç«¯ 2: å¯åŠ¨å¾®è°ƒæ¨¡å‹æœåŠ¡ (ç«¯å£ 8001)
llamafactory-cli api \
  --model_name_or_path ./Qwen/Qwen3-8B \
  --adapter_name_or_path ./saves/qwen3_mind_news_recommend \
  --template qwen \
  --finetuning_type lora \
  --port 8001

# æœåŠ¡å¯åŠ¨åè¾“å‡º:
# INFO: Application startup complete.
# INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

#### æ­¥éª¤ 2: ä½¿ç”¨ API è¯„ä¼°

æ‰“å¼€ [æ¨¡å‹å¯¹æ¯”è¯„ä¼°.ipynb](æ¨¡å‹å¯¹æ¯”è¯„ä¼°.ipynb):

```python
from openai import OpenAI
import json

# 1. åˆ›å»º API å®¢æˆ·ç«¯
base_client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8000/v1"
)

finetuned_client = OpenAI(
    api_key="EMPTY",
    base_url="http://localhost:8001/v1"
)

# 2. åŠ è½½æµ‹è¯•æ•°æ®
with open("data/processed/mind_test.json", "r", encoding="utf-8") as f:
    test_data = [json.loads(line) for line in f][:100]  # æµ‹è¯• 100 æ ·æœ¬

# 3. å•æ¡æ¨ç†å‡½æ•°
def predict(client, sample):
    """è°ƒç”¨ API è¿›è¡Œæ¨ç†"""
    response = client.chat.completions.create(
        model="qwen3-8B",
        messages=[
            {"role": "system", "content": sample["instruction"]},
            {"role": "user", "content": sample["input"]}
        ],
        temperature=0.01,
        max_tokens=10
    )
    return response.choices[0].message.content

# 4. è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
base_correct = 0
finetuned_correct = 0

for i, sample in enumerate(test_data):
    if i % 10 == 0:
        print(f"è¿›åº¦: {i}/{len(test_data)}")

    # åŸºåº§æ¨¡å‹é¢„æµ‹
    base_pred = predict(base_client, sample)
    if base_pred.strip() == sample["output"]:
        base_correct += 1

    # å¾®è°ƒæ¨¡å‹é¢„æµ‹
    finetuned_pred = predict(finetuned_client, sample)
    if finetuned_pred.strip() == sample["output"]:
        finetuned_correct += 1

print(f"\nåŸºåº§æ¨¡å‹å‡†ç¡®ç‡: {base_correct/len(test_data)*100:.2f}%")
print(f"å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡: {finetuned_correct/len(test_data)*100:.2f}%")
```

#### cURL è°ƒç”¨ç¤ºä¾‹

```bash
# è°ƒç”¨åŸºåº§æ¨¡å‹
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-8B",
    "messages": [
      {"role": "system", "content": "You are a news recommendation expert."},
      {"role": "user", "content": "User click histories: \"News A\", \"News B\"\nTarget news: \"News C\"\nWill the user click? Answer with Yes. or No."}
    ],
    "temperature": 0.01,
    "max_tokens": 10
  }'
```

---

### æ€§èƒ½å¯¹æ¯”è¡¨æ ¼

#### vLLM vs API æ¨ç†é€Ÿåº¦å¯¹æ¯”

| æ ·æœ¬æ•°é‡ | vLLM è€—æ—¶ | API è€—æ—¶ | åŠ é€Ÿå€æ•° |
|---------|----------|---------|---------|
| **100** | 10-20 ç§’ | 3-5 åˆ†é’Ÿ | **15-20x** |
| **1,000** | 2-5 åˆ†é’Ÿ | 30-50 åˆ†é’Ÿ | **18-25x** |
| **10,000** | 10-20 åˆ†é’Ÿ | 5-8 å°æ—¶ | **20-25x** |
| **31,000 (å…¨é‡)** | 25-50 åˆ†é’Ÿ | 20+ å°æ—¶ | **25-30x** |

#### åŸºåº§æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”

| æ•°æ®é›† | åŸºåº§æ¨¡å‹ | å¾®è°ƒæ¨¡å‹ (LoRA) | å‡†ç¡®ç‡æå‡ |
|--------|---------|----------------|-----------|
| **éªŒè¯é›†** (10%) | ~52% | ~68% | **+16%** |
| **æµ‹è¯•é›†** (1,000) | ~51% | ~67% | **+16%** |
| **æµ‹è¯•é›†** (å…¨é‡ 31K) | ~50% | ~66% | **+16%** |

*æ³¨: å…·ä½“æ•°å€¼æ ¹æ®è®­ç»ƒå‚æ•°å’Œæ•°æ®åˆ’åˆ†ç•¥æœ‰ä¸åŒ*

#### ä¸åŒæ‰¹æ¬¡å¤§å°çš„ååé‡å¯¹æ¯” (vLLM)

| æ‰¹æ¬¡å¤§å° | ååé‡ (samples/sec) | æ˜¾å­˜å ç”¨ | å»¶è¿Ÿ (ms/sample) |
|---------|---------------------|---------|-----------------|
| 1 | 5-8 | 20GB | 125-200 |
| 8 | 15-20 | 24GB | 50-65 |
| 16 | 20-25 | 26GB | 40-50 |
| 32 | 25-30 | 28GB | 33-40 |

---

### è¯„ä¼°æŒ‡æ ‡è¯´æ˜

#### 1. å‡†ç¡®ç‡ (Accuracy)

```python
accuracy = correct_predictions / total_samples
```

- **åŸºåº§æ¨¡å‹**: ~50% (æ¥è¿‘éšæœºçŒœæµ‹)
- **å¾®è°ƒæ¨¡å‹**: ~66-68% (æ˜¾è‘—æå‡)

#### 2. æ¨ç†ååé‡ (Samples/sec)

æ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°é‡:

- **vLLM**: 10-30 samples/sec
- **API**: 0.3-0.5 samples/sec

#### 3. å¹³å‡å»¶è¿Ÿ (ms/sample)

å•ä¸ªæ ·æœ¬çš„æ¨ç†æ—¶é—´:

- **vLLM**: 33-125 ms
- **API**: 2000-3000 ms

#### 4. åŠ é€Ÿå€æ•°

vLLM ç›¸å¯¹äº API çš„åŠ é€Ÿ:

```
åŠ é€Ÿå€æ•° = APIæ¨ç†æ—¶é—´ / vLLMæ¨ç†æ—¶é—´
```

- **å¹³å‡åŠ é€Ÿ**: **18-25 å€**

---

## ğŸ’» ä½¿ç”¨æŒ‡å—

### Python API æ¨ç† (vLLM)

#### å•æ¡æ¨ç†ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# 1. åŠ è½½æ¨¡å‹
llm = LLM(
    model="./merged_models/qwen3_mind_news_recommend",
    trust_remote_code=True,
    gpu_memory_utilization=0.9
)

# 2. å®šä¹‰æ¨ç†å‚æ•°
sampling_params = SamplingParams(
    temperature=0.01,
    top_p=0.9,
    max_tokens=10,
    stop=["<|im_end|>", "\n"]
)

# 3. æ„å»º prompt
prompt = """<|im_start|>system
You are a news recommendation expert. Based on user's click history and preferences, predict if they will click the target news.<|im_end|>
<|im_start|>user
User click histories: "Tesla unveils new Model Y", "SpaceX launches Starship"
User's liked news: "Apple announces iPhone 16", "Google's new AI model"
User's disliked news: "Local weather update", "Sports scores"
Target news: "Microsoft acquires AI startup"
Will the user click this news? Answer with 'Yes.' or 'No.'<|im_end|>
<|im_start|>assistant
"""

# 4. æ¨ç†
outputs = llm.generate([prompt], sampling_params)
prediction = outputs[0].outputs[0].text.strip()
print(f"é¢„æµ‹ç»“æœ: {prediction}")  # è¾“å‡º: Yes. æˆ– No.
```

#### æ‰¹é‡æ¨ç†ç¤ºä¾‹

```python
# æ‰¹é‡æ¨ç† (vLLM è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†)
prompts = [
    build_prompt(sample1),
    build_prompt(sample2),
    build_prompt(sample3),
    # ... æ›´å¤šæ ·æœ¬
]

# ä¸€æ¬¡æ€§æ¨ç†æ‰€æœ‰æ ·æœ¬ (è‡ªåŠ¨æ‰¹å¤„ç†)
outputs = llm.generate(prompts, sampling_params)

# è§£æç»“æœ
predictions = [output.outputs[0].text.strip() for output in outputs]
```

---

### HTTP API æ¨ç†

#### å¯åŠ¨ API æœåŠ¡

```bash
# å¯åŠ¨å¾®è°ƒæ¨¡å‹æœåŠ¡
llamafactory-cli api \
  --model_name_or_path ./Qwen/Qwen3-8B \
  --adapter_name_or_path ./saves/qwen3_mind_news_recommend \
  --template qwen \
  --finetuning_type lora \
  --port 8000
```

#### cURL è°ƒç”¨ç¤ºä¾‹

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-8B",
    "messages": [
      {
        "role": "system",
        "content": "You are a news recommendation expert. Based on user'\''s click history and preferences, predict if they will click the target news."
      },
      {
        "role": "user",
        "content": "User click histories: \"Tesla unveils new Model Y\", \"SpaceX launches Starship\"\nUser'\''s liked news: \"Apple announces iPhone 16\", \"Google'\''s new AI model\"\nUser'\''s disliked news: \"Local weather update\", \"Sports scores\"\nTarget news: \"Microsoft acquires AI startup\"\nWill the user click this news? Answer with '\''Yes.'\'' or '\''No.'\''."
      }
    ],
    "temperature": 0.01,
    "max_tokens": 10
  }'

# å“åº”ç¤ºä¾‹:
# {
#   "id": "chatcmpl-xxx",
#   "object": "chat.completion",
#   "created": 1234567890,
#   "model": "qwen3-8B",
#   "choices": [
#     {
#       "index": 0,
#       "message": {
#         "role": "assistant",
#         "content": "Yes."
#       },
#       "finish_reason": "stop"
#     }
#   ]
# }
```

#### Python requests è°ƒç”¨ç¤ºä¾‹

```python
import requests
import json

url = "http://localhost:8000/v1/chat/completions"

payload = {
    "model": "qwen3-8B",
    "messages": [
        {
            "role": "system",
            "content": "You are a news recommendation expert. Based on user's click history and preferences, predict if they will click the target news."
        },
        {
            "role": "user",
            "content": """User click histories: "Tesla unveils new Model Y", "SpaceX launches Starship"
User's liked news: "Apple announces iPhone 16", "Google's new AI model"
User's disliked news: "Local weather update", "Sports scores"
Target news: "Microsoft acquires AI startup"
Will the user click this news? Answer with 'Yes.' or 'No.'."""
        }
    ],
    "temperature": 0.01,
    "max_tokens": 10
}

response = requests.post(url, json=payload)
result = response.json()
prediction = result["choices"][0]["message"]["content"]
print(f"é¢„æµ‹ç»“æœ: {prediction}")
```

#### OpenAI SDK è°ƒç”¨ç¤ºä¾‹

```python
from openai import OpenAI

# åˆ›å»ºå®¢æˆ·ç«¯
client = OpenAI(
    api_key="EMPTY",  # LLaMA-Factory ä¸éœ€è¦ API key
    base_url="http://localhost:8000/v1"
)

# è°ƒç”¨æ¨¡å‹
response = client.chat.completions.create(
    model="qwen3-8B",
    messages=[
        {
            "role": "system",
            "content": "You are a news recommendation expert. Based on user's click history and preferences, predict if they will click the target news."
        },
        {
            "role": "user",
            "content": """User click histories: "Tesla unveils new Model Y", "SpaceX launches Starship"
User's liked news: "Apple announces iPhone 16", "Google's new AI model"
User's disliked news: "Local weather update", "Sports scores"
Target news: "Microsoft acquires AI startup"
Will the user click this news? Answer with 'Yes.' or 'No.'."""
        }
    ],
    temperature=0.01,
    max_tokens=10
)

prediction = response.choices[0].message.content
print(f"é¢„æµ‹ç»“æœ: {prediction}")
```

---

### Prompt æ„å»ºè¯´æ˜

#### æ¨èçš„ Prompt æ ¼å¼

```python
def build_recommendation_prompt(
    history_titles: list,  # ç”¨æˆ·å†å²ç‚¹å‡»
    liked_titles: list,    # å–œæ¬¢çš„æ–°é—»
    disliked_titles: list, # ä¸å–œæ¬¢çš„æ–°é—»
    target_title: str      # ç›®æ ‡æ–°é—»
):
    """æ„å»ºæ¨èä»»åŠ¡çš„ prompt"""

    # æ ¼å¼åŒ–åˆ—è¡¨
    history_str = ', '.join([f'"{t}"' for t in history_titles])
    liked_str = ', '.join([f'"{t}"' for t in liked_titles[:3]])  # æœ€å¤š3æ¡
    disliked_str = ', '.join([f'"{t}"' for t in disliked_titles[:3]])

    # æ„å»º prompt
    system_prompt = "You are a news recommendation expert. Based on user's click history and preferences, predict if they will click the target news."

    user_prompt = f"""User click histories: {history_str}
User's liked news: {liked_str}
User's disliked news: {disliked_str}
Target news: "{target_title}"
Will the user click this news? Answer with 'Yes.' or 'No.'."""

    # Qwen3 æ ¼å¼ (ç”¨äº vLLM)
    qwen_prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""

    return qwen_prompt
```

#### å“åº”è§£æ

```python
def parse_prediction(response_text: str) -> str:
    """è§£ææ¨¡å‹è¾“å‡º"""
    text = response_text.strip().lower()

    if "yes" in text:
        return "Yes."
    elif "no" in text:
        return "No."
    else:
        return "Unknown"  # æ— æ³•è¯†åˆ«çš„è¾“å‡º
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
4.ChatGLM å¾®è°ƒæ¨èæ¨¡å‹/
â”‚
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ MIND/                       # åŸå§‹ MIND æ•°æ®é›†
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”‚   â”œâ”€â”€ news.tsv            # è®­ç»ƒé›†æ–°é—» (101,527æ¡)
â”‚   â”‚   â”‚   â””â”€â”€ behaviors.tsv       # è®­ç»ƒé›†ç”¨æˆ·è¡Œä¸º (2,232,748æ¡)
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â”‚   â”œâ”€â”€ news.tsv
â”‚   â”‚   â”‚   â””â”€â”€ behaviors.tsv
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”‚       â”œâ”€â”€ news.tsv            # æµ‹è¯•é›†æ–°é—» (120,961æ¡)
â”‚   â”‚       â””â”€â”€ behaviors.tsv       # æµ‹è¯•é›†ç”¨æˆ·è¡Œä¸º (2,370,727æ¡)
â”‚   â””â”€â”€ processed/                  # å¤„ç†åçš„æ•°æ®
â”‚       â”œâ”€â”€ mind_train.json         # è®­ç»ƒæ ·æœ¬ (~125,000æ¡, 753KB)
â”‚       â”œâ”€â”€ mind_val.json           # éªŒè¯æ ·æœ¬ (~35,000æ¡)
â”‚       â””â”€â”€ mind_test.json          # æµ‹è¯•æ ·æœ¬ (~31,000æ¡, 188KB)
â”‚
â”œâ”€â”€ Qwen/                           # æ¨¡å‹ç›®å½•
â”‚   â””â”€â”€ Qwen3-8B/                  # åŸºåº§æ¨¡å‹ (~28GB)
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model-00001-of-00008.safetensors
â”‚       â”œâ”€â”€ ...
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â””â”€â”€ tokenizer_config.json
â”‚
â”œâ”€â”€ saves/                          # è®­ç»ƒè¾“å‡º
â”‚   â””â”€â”€ qwen3_mind_news_recommend/  # LoRA é€‚é…å™¨ (84MB)
â”‚       â”œâ”€â”€ adapter_model.safetensors   # LoRA æƒé‡ (81MB)
â”‚       â”œâ”€â”€ adapter_config.json         # LoRA é…ç½®
â”‚       â”œâ”€â”€ trainer_state.json          # è®­ç»ƒçŠ¶æ€
â”‚       â”œâ”€â”€ training_args.bin           # è®­ç»ƒå‚æ•°
â”‚       â””â”€â”€ checkpoint-*/               # ä¸­é—´æ£€æŸ¥ç‚¹
â”‚
â”œâ”€â”€ merged_models/                  # åˆå¹¶åçš„æ¨¡å‹ (vLLM ä½¿ç”¨)
â”‚   â””â”€â”€ qwen3_mind_news_recommend/  # å®Œæ•´å¾®è°ƒæ¨¡å‹ (~28GB)
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model-00001-of-00008.safetensors
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ tokenizer.json
â”‚
â”œâ”€â”€ æ•°æ®é¢„å¤„ç†.ipynb                 # æ•°æ®å¤„ç† Notebook (38KB)
â”œâ”€â”€ æ¨¡å‹å¯¹æ¯”è¯„ä¼°_vllm.ipynb          # vLLM è¯„ä¼° (æ¨è, 23KB)
â”œâ”€â”€ æ¨¡å‹å¯¹æ¯”è¯„ä¼°.ipynb               # API è¯„ä¼° (å¤‡é€‰, 7KB)
â”œâ”€â”€ æ¨¡å‹é¢„æµ‹.ipynb                   # åŸæœ‰æ¨ç†è„šæœ¬ (184KB)
â”‚
â”œâ”€â”€ download_qwen_model.py          # æ¨¡å‹ä¸‹è½½è„šæœ¬ (2.4KB)
â”œâ”€â”€ merge_lora_weights.py           # LoRA æƒé‡åˆå¹¶ (3.4KB)
â”‚
â”œâ”€â”€ new_train.yaml                  # è®­ç»ƒé…ç½®
â”œâ”€â”€ news_inference.yaml             # æ¨ç†é…ç½®
â”œâ”€â”€ dataset_info.json               # æ•°æ®é›†æ³¨å†Œ
â”‚
â”œâ”€â”€ CLAUDE.md                       # é¡¹ç›®æ–‡æ¡£ (9.1KB)
â”œâ”€â”€ README.md                       # æœ¬æ–‡ä»¶
â”‚
â””â”€â”€ å‚è€ƒèµ„æ–™/
    â”œâ”€â”€ A Survey on Large Language Models for Recommendation.pdf  (2.1MB)
    â””â”€â”€ é¡¹ç›®æ‰‹å†Œï¼šSFTæ¨èæ¨¡å‹å¾®è°ƒ.pdf  (604KB)
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### è®­ç»ƒä¼˜åŒ–

#### 1. æ˜¾å­˜ä¼˜åŒ–

```yaml
# æ–¹æ³• 1: ä½¿ç”¨é‡åŒ–
quantization_bit: 4  # 4-bit é‡åŒ–, æ˜¾å­˜å‡åŠ

# æ–¹æ³• 2: å‡å°‘æ‰¹å¤§å°
per_device_train_batch_size: 1
gradient_accumulation_steps: 16  # ä¿æŒæœ‰æ•ˆæ‰¹å¤§å°

# æ–¹æ³• 3: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
gradient_checkpointing: true  # æ˜¾å­˜æ¢æ—¶é—´

# æ–¹æ³• 4: å‡å°‘åºåˆ—é•¿åº¦
cutoff_len: 512  # ä» 1024 é™ä½

# æ–¹æ³• 5: å‡å°‘ LoRA ç›®æ ‡æ¨¡å—
lora_target: q_proj,v_proj  # åªä¿ç•™æ ¸å¿ƒæ¨¡å—
```

#### 2. è®­ç»ƒåŠ é€Ÿ

```yaml
# æ–¹æ³• 1: å¯ç”¨æ··åˆç²¾åº¦
bf16: true  # BF16 æ··åˆç²¾åº¦

# æ–¹æ³• 2: ä½¿ç”¨èåˆä¼˜åŒ–å™¨
optim: adamw_torch_fused

# æ–¹æ³• 3: å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
preprocessing_num_workers: 16

# æ–¹æ³• 4: ç¦ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
gradient_checkpointing: false  # æ—¶é—´æ¢æ˜¾å­˜
```

#### 3. å¤šå¡è®­ç»ƒ (å¦‚æœæœ‰å¤šå¼  GPU)

```bash
# ä½¿ç”¨ DeepSpeed æˆ– FSDP
llamafactory-cli train new_train.yaml --deepspeed ds_config.json

# æˆ–ä½¿ç”¨ torchrun
torchrun --nproc_per_node 2 -m llmtuner.train new_train.yaml
```

### æ¨ç†ä¼˜åŒ–

#### 1. vLLM å‚æ•°è°ƒä¼˜

```python
from vllm import LLM

llm = LLM(
    model="./merged_models/qwen3_mind_news_recommend",

    # æ˜¾å­˜ç®¡ç†
    gpu_memory_utilization=0.9,  # æ¨è 0.85-0.95
    max_model_len=2048,          # æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´

    # æ‰¹å¤„ç†ä¼˜åŒ–
    max_num_batched_tokens=8192, # å¢åŠ æ‰¹å¤„ç†ä¸Šé™
    max_num_seqs=256,            # å¢åŠ å¹¶å‘åºåˆ—æ•°

    # æ€§èƒ½ä¼˜åŒ–
    dtype="bfloat16",            # ä½¿ç”¨ BF16
    enforce_eager=False,         # å¯ç”¨ CUDA graph
    trust_remote_code=True
)
```

#### 2. æ‰¹é‡æ¨ç†ç­–ç•¥

```python
# å°†å¤§é‡æ ·æœ¬åˆ†æ‰¹å¤„ç†
def batch_inference(llm, prompts, batch_size=128):
    """åˆ†æ‰¹æ¨ç†,é¿å…æ˜¾å­˜æº¢å‡º"""
    results = []
    for i in range(0, len(prompts), batch_size):
        batch = prompts[i:i+batch_size]
        outputs = llm.generate(batch, sampling_params)
        results.extend(outputs)
    return results
```

#### 3. é‡åŒ–æ¨ç† (é™ä½æ˜¾å­˜)

```python
# ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¨ç†
from vllm import LLM

llm = LLM(
    model="./merged_models/qwen3_mind_news_recommend",
    quantization="awq",  # æˆ– "gptq", "squeezellm"
    dtype="float16"
)
```

---

## â“ å¸¸è§é—®é¢˜ (FAQ)

### è®­ç»ƒç›¸å…³

#### Q1: CUDA Out of Memory (æ˜¾å­˜ä¸è¶³)

**é—®é¢˜**: è®­ç»ƒæ—¶æŠ¥é”™ `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
1. **é™ä½æ‰¹å¤§å°**: `per_device_train_batch_size: 1`
2. **å¢åŠ æ¢¯åº¦ç´¯ç§¯**: `gradient_accumulation_steps: 16`
3. **ä½¿ç”¨é‡åŒ–**: `quantization_bit: 4`
4. **å‡å°‘åºåˆ—é•¿åº¦**: `cutoff_len: 512`
5. **å‡å°‘ LoRA ç›®æ ‡**: `lora_target: q_proj,v_proj`
6. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**: `gradient_checkpointing: true`

#### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**é—®é¢˜**: è®­ç»ƒ 1 ä¸ª epoch éœ€è¦å¾ˆé•¿æ—¶é—´

**è§£å†³æ–¹æ¡ˆ**:
1. **å¯ç”¨æ··åˆç²¾åº¦**: `bf16: true`
2. **å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹**: `preprocessing_num_workers: 16`
3. **ä½¿ç”¨èåˆä¼˜åŒ–å™¨**: `optim: adamw_torch_fused`
4. **å‡å°‘è¯„ä¼°é¢‘ç‡**: `eval_steps: 1000`
5. **é™åˆ¶è®­ç»ƒæ ·æœ¬**: `max_samples: 50000`

#### Q3: éªŒè¯æŸå¤±ä¸ä¸‹é™

**é—®é¢˜**: è®­ç»ƒæŸå¤±ä¸‹é™, ä½†éªŒè¯æŸå¤±ä¸é™æˆ–ä¸Šå‡

**è§£å†³æ–¹æ¡ˆ**:
1. **é™ä½å­¦ä¹ ç‡**: `learning_rate: 5.0e-5`
2. **å¢åŠ è®­ç»ƒæ•°æ®**: åˆ é™¤ `max_samples` é™åˆ¶
3. **è°ƒæ•´æ­£åˆ™åŒ–**: å¢åŠ  `weight_decay: 0.01`
4. **æ£€æŸ¥æ•°æ®è´¨é‡**: ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†å¸ƒä¸€è‡´
5. **å¢åŠ è®­ç»ƒè½®æ•°**: `num_train_epochs: 5.0`

### æ¨ç†ç›¸å…³

#### Q4: vLLM åŠ è½½å¤±è´¥

**é—®é¢˜**: `RuntimeError: Failed to load model` æˆ– `No module named 'vllm'`

**è§£å†³æ–¹æ¡ˆ**:
1. **æ£€æŸ¥ vLLM å®‰è£…**: `pip install vllm`
2. **ç¡®è®¤æƒé‡å·²åˆå¹¶**: è¿è¡Œ `python merge_lora_weights.py`
3. **æ£€æŸ¥æ¨¡å‹è·¯å¾„**: ç¡®ä¿ `./merged_models/qwen3_mind_news_recommend/` å­˜åœ¨
4. **æ£€æŸ¥ CUDA ç‰ˆæœ¬**: vLLM éœ€è¦ CUDA 11.8+
5. **é™ä½æ˜¾å­˜å ç”¨**: `gpu_memory_utilization=0.7`

#### Q5: vLLM æ¨ç†é€Ÿåº¦æ…¢

**é—®é¢˜**: vLLM æ¨ç†é€Ÿåº¦æœªè¾¾åˆ°é¢„æœŸ

**è§£å†³æ–¹æ¡ˆ**:
1. **å¢åŠ æ‰¹å¤„ç†ä¸Šé™**: `max_num_batched_tokens=16384`
2. **å¢åŠ å¹¶å‘åºåˆ—**: `max_num_seqs=512`
3. **ä½¿ç”¨ CUDA graph**: `enforce_eager=False`
4. **æ£€æŸ¥æ‰¹é‡å¤§å°**: ç¡®ä¿ä¸€æ¬¡æ¨ç†å¤šä¸ªæ ·æœ¬
5. **ä½¿ç”¨é‡åŒ–**: `quantization="awq"`

#### Q6: API æœåŠ¡æ— æ³•å¯åŠ¨

**é—®é¢˜**: `llamafactory-cli api` å¯åŠ¨å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
1. **æ£€æŸ¥ç«¯å£å ç”¨**: `lsof -i :8000` (Linux/Mac) æˆ– `netstat -ano | findstr 8000` (Windows)
2. **æ›´æ¢ç«¯å£**: `--port 8001`
3. **æ£€æŸ¥é…ç½®æ–‡ä»¶**: ç¡®ä¿ `news_inference.yaml` è·¯å¾„æ­£ç¡®
4. **æŸ¥çœ‹é”™è¯¯æ—¥å¿—**: æ£€æŸ¥ç»ˆç«¯è¾“å‡ºçš„è¯¦ç»†é”™è¯¯ä¿¡æ¯

### æ•°æ®ç›¸å…³

#### Q7: æ•°æ®å¤„ç†å†…å­˜ä¸è¶³

**é—®é¢˜**: è¿è¡Œ `æ•°æ®é¢„å¤„ç†.ipynb` æ—¶å†…å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆ**:
1. **åˆ†æ‰¹å¤„ç†**: å°†æ•°æ®é›†åˆ†æˆå¤šä¸ªå°æ–‡ä»¶å¤„ç†
2. **ä½¿ç”¨ç”Ÿæˆå™¨**: é¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
3. **å¢åŠ ç³»ç»Ÿå†…å­˜**: è‡³å°‘ 16GB RAM
4. **æ¸…ç†ä¸­é—´å˜é‡**: ä½¿ç”¨ `del` å’Œ `gc.collect()`

#### Q8: æ ·æœ¬æ•°é‡ä¸ç¬¦åˆé¢„æœŸ

**é—®é¢˜**: å¤„ç†åçš„æ ·æœ¬æ•°é‡ä¸æ–‡æ¡£ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**:
1. **æ£€æŸ¥æ•°æ®å®Œæ•´æ€§**: ç¡®ä¿ `news.tsv` å’Œ `behaviors.tsv` å®Œæ•´
2. **è°ƒæ•´è¿‡æ»¤è§„åˆ™**: æ£€æŸ¥ä»£ç ä¸­çš„æ ·æœ¬è¿‡æ»¤æ¡ä»¶
3. **æŸ¥çœ‹æ—¥å¿—**: ç¡®è®¤å¤„ç†è¿‡ç¨‹ä¸­æ˜¯å¦æœ‰é”™è¯¯æˆ–è­¦å‘Š

### æ¨¡å‹ç›¸å…³

#### Q9: æ¨¡å‹å‡†ç¡®ç‡ä½äºé¢„æœŸ

**é—®é¢˜**: å¾®è°ƒåçš„æ¨¡å‹å‡†ç¡®ç‡ä»ç„¶å¾ˆä½ (<60%)

**è§£å†³æ–¹æ¡ˆ**:
1. **å¢åŠ è®­ç»ƒè½®æ•°**: `num_train_epochs: 5.0`
2. **å¢åŠ è®­ç»ƒæ•°æ®**: ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ® (åˆ é™¤ `max_samples`)
3. **è°ƒæ•´å­¦ä¹ ç‡**: å°è¯• `learning_rate: 5.0e-5` æˆ– `2.0e-4`
4. **å¢åŠ  LoRA rank**: ä¿®æ”¹ `adapter_config.json` ä¸­çš„ `r: 16`
5. **æ£€æŸ¥æ•°æ®è´¨é‡**: ç¡®ä¿æ ·æœ¬æ ‡ç­¾æ­£ç¡®, æ— æ•°æ®æ³„éœ²

#### Q10: å¾®è°ƒå‰åæ•ˆæœæ²¡æœ‰æå‡

**é—®é¢˜**: åŸºåº§æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹å‡†ç¡®ç‡ç›¸è¿‘

**è§£å†³æ–¹æ¡ˆ**:
1. **æ£€æŸ¥æ¨¡å‹åŠ è½½**: ç¡®ä¿è¯„ä¼°æ—¶åŠ è½½äº†æ­£ç¡®çš„å¾®è°ƒæ¨¡å‹
2. **æ£€æŸ¥ LoRA æƒé‡**: ç¡®è®¤ `adapter_model.safetensors` æ–‡ä»¶å­˜åœ¨ä¸”å¤§å°æ­£å¸¸
3. **æ£€æŸ¥è¯„ä¼°ä»£ç **: ç¡®ä¿ä½¿ç”¨äº†æ­£ç¡®çš„ prompt æ ¼å¼
4. **é‡æ–°è®­ç»ƒ**: å°è¯•ä»å¤´é‡æ–°è®­ç»ƒ, è°ƒæ•´è¶…å‚æ•°

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡

1. **[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)**
   Edward J. Hu, Yelong Shen, et al. (2021)
   *LoRA åŸå§‹è®ºæ–‡, ä»‹ç»äº†å‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ ¸å¿ƒæ€æƒ³*

2. **[A Survey on Large Language Models for Recommendation](A%20Survey%20on%20Large%20Language%20Models%20for%20Recommendation.pdf)**
   *ç»¼è¿°è®ºæ–‡, è¯¦ç»†ä»‹ç»äº† LLM åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨*

3. **[MIND: A Large-scale Dataset for News Recommendation](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf)**
   Fangzhao Wu, et al. (ACL 2020)
   *MIND æ•°æ®é›†è®ºæ–‡*

4. **[vLLM: Easy, Fast, and Cheap LLM Serving](https://arxiv.org/abs/2309.06180)**
   Woosuk Kwon, et al. (2023)
   *vLLM æ¨ç†å¼•æ“è®ºæ–‡*

### å¼€æºé¡¹ç›®

- **[Qwen](https://github.com/QwenLM/Qwen)**: é€šä¹‰åƒé—®å¤§æ¨¡å‹
- **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)**: é«˜æ•ˆçš„ LLM å¾®è°ƒæ¡†æ¶
- **[vLLM](https://github.com/vllm-project/vllm)**: é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“
- **[PEFT](https://github.com/huggingface/peft)**: Hugging Face å‚æ•°é«˜æ•ˆå¾®è°ƒåº“

### æ–‡æ¡£å’Œæ•™ç¨‹

- **[Qwen3 å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen/blob/main/README_CN.md)**
- **[LLaMA-Factory ä½¿ç”¨æŒ‡å—](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md)**
- **[vLLM å¿«é€Ÿå¼€å§‹](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)**
- **[MIND æ•°æ®é›†å®˜ç½‘](https://msnews.github.io/)**


---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œå›¢é˜Ÿ:

- **é˜¿é‡Œå·´å·´é€šä¹‰åƒé—®å›¢é˜Ÿ**: å¼€æº Qwen3-8B æ¨¡å‹
- **å¾®è½¯ç ”ç©¶é™¢**: å‘å¸ƒ MIND æ•°æ®é›†
- **LLaMA-Factory å›¢é˜Ÿ**: æä¾›é«˜æ•ˆçš„å¾®è°ƒæ¡†æ¶
- **vLLM å›¢é˜Ÿ**: å¼€å‘é«˜æ€§èƒ½æ¨ç†å¼•æ“
- **Hugging Face**: PEFT åº“å’Œç”Ÿæ€ç³»ç»Ÿ

---

<div align="center">

**â­ å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©, æ¬¢è¿ Star æ”¯æŒ! â­**


</div>
